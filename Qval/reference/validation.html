<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Perform Q-matrix Validation Methods — validation • Qval</title><!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous"><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css"><script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../pkgdown.js"></script><meta property="og:title" content="Perform Q-matrix Validation Methods — validation"><meta property="og:description" content="This function uses generalized Q-matrix validation methods to validate the Q-matrix,
including commonly used methods such as GDI (de la Torre, &amp;amp; Chiu, 2016; Najera, Sorrel,
&amp;amp; Abad, 2019; Najera et al., 2020), Wald (Ma, &amp;amp; de la Torre, 2020), Hull (Najera et al.,
2021), and MLR-B (Tu et al., 2022). It supports different iteration methods (test
level or item level; Najera et al., 2020; Najera et al., 2021; Tu et al., 2022) and
can apply various attribute search methods (ESA, SSA, PAA; de la Torre, 2008; Terzi, &amp;amp;
de la Torre, 2018; Qin &amp;amp; Guo, 2025)."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body data-spy="scroll" data-target="#toc">


    <div class="container template-reference-topic">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">Qval</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">1.2.4</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"></ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Perform Q-matrix Validation Methods</h1>

    <div class="hidden name"><code>validation.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>This function uses generalized Q-matrix validation methods to validate the Q-matrix,
including commonly used methods such as GDI (de la Torre, &amp; Chiu, 2016; Najera, Sorrel,
&amp; Abad, 2019; Najera et al., 2020), Wald (Ma, &amp; de la Torre, 2020), Hull (Najera et al.,
2021), and MLR-B (Tu et al., 2022). It supports different iteration methods (test
level or item level; Najera et al., 2020; Najera et al., 2021; Tu et al., 2022) and
can apply various attribute search methods (ESA, SSA, PAA; de la Torre, 2008; Terzi, &amp;
de la Torre, 2018; Qin &amp; Guo, 2025).</p>
    </div>

    <div id="ref-usage">
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">validation</span><span class="op">(</span></span>
<span>  <span class="va">Y</span>,</span>
<span>  <span class="va">Q</span>,</span>
<span>  CDM.obj <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  par.method <span class="op">=</span> <span class="st">"EM"</span>,</span>
<span>  mono.constraint <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"GDINA"</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"GDI"</span>,</span>
<span>  search.method <span class="op">=</span> <span class="st">"PAA"</span>,</span>
<span>  iter.level <span class="op">=</span> <span class="st">"no"</span>,</span>
<span>  maxitr <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  eps <span class="op">=</span> <span class="fl">0.95</span>,</span>
<span>  alpha.level <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  criter <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div id="arguments">
    <h2>Arguments</h2>


<dl><dt id="arg-y">Y<a class="anchor" aria-label="anchor" href="#arg-y"></a></dt>
<dd><p>A required \(N\) × \(I\) matrix or <code>data.frame</code> consisting of the responses of <code>N</code> individuals
to \(N\) × \(I\) items. Missing values need to be coded as <code>NA</code>.</p></dd>


<dt id="arg-q">Q<a class="anchor" aria-label="anchor" href="#arg-q"></a></dt>
<dd><p>A required binary \(I\) × \(K\) matrix containing the attributes not required or required
master the items. The <code>i</code>th row of the matrix is a binary indicator vector indicating which
attributes are not required (coded by 0) and which attributes are required (coded by 1) to master
item \(i\).</p></dd>


<dt id="arg-cdm-obj">CDM.obj<a class="anchor" aria-label="anchor" href="#arg-cdm-obj"></a></dt>
<dd><p>An object of class <code>CDM.obj</code>. When it is not NULL, it enables rapid validation
of the Q-matrix without the need for parameter estimation. @seealso <code><a href="CDM.html">CDM</a></code>.</p></dd>


<dt id="arg-par-method">par.method<a class="anchor" aria-label="anchor" href="#arg-par-method"></a></dt>
<dd><p>Type of method to estimate CDMs' parameters; one out of <code>"EM"</code>, <code>"BM"</code>. Default = <code>"EM"</code>.
However, <code>"BM"</code> is only available when <code>method = "GDINA"</code>.</p></dd>


<dt id="arg-mono-constraint">mono.constraint<a class="anchor" aria-label="anchor" href="#arg-mono-constraint"></a></dt>
<dd><p>Logical indicating whether monotonicity constraints should be fulfilled in estimation.
Default = <code>FALSE</code>.</p></dd>


<dt id="arg-model">model<a class="anchor" aria-label="anchor" href="#arg-model"></a></dt>
<dd><p>Type of model to fit; can be <code>"GDINA"</code>, <code>"LCDM"</code>, <code>"DINA"</code>, <code>"DINO"</code>
, <code>"ACDM"</code>, <code>"LLM"</code>, or <code>"rRUM"</code>. Default = <code>"GDINA"</code>.
@seealso <code><a href="CDM.html">CDM</a></code>.</p></dd>


<dt id="arg-method">method<a class="anchor" aria-label="anchor" href="#arg-method"></a></dt>
<dd><p>The methods to validata Q-matrix, can be <code>"GDI"</code>, <code>"Wald"</code>, <code>"Hull"</code>,
<code>"MLR-B"</code> and <code>"beta"</code>. The <code>"model"</code> must be <code>"GDINA"</code> when
<code>method = "Wald"</code>. Please note that the \(\beta\) method has different meanings
when applying different search algorithms. For more details, see section 'Search algorithm' below.
Default = <code>"GDI"</code>. See details.</p></dd>


<dt id="arg-search-method">search.method<a class="anchor" aria-label="anchor" href="#arg-search-method"></a></dt>
<dd><p>Character string specifying the search method to use during validation.</p><dl><dt><code>"ESA"</code></dt>
<dd><p>for exhaustive search algorithm. Cannot be used with the <code>"Wald"</code> method.</p></dd>

  <dt><code>"SSA"</code></dt>
<dd><p>for sequential search algorithm (see de la Torre, 2008; Terzi &amp; de la Torre, 2018).
                      It will be equal to <code>"forward"</code> when <code>method = "Wald"</code>.</p></dd>

  <dt><code>"PAA"</code></dt>
<dd><p>for priority attribute algorithm.</p></dd>

  <dt><code>"stepwise"</code></dt>
<dd><p>only for the <code>"Wald"</code> method</p></dd>

  <dt><code>"beta"</code></dt>
<dd><p>only for the <code>"beta"</code> method</p></dd>


</dl></dd>


<dt id="arg-iter-level">iter.level<a class="anchor" aria-label="anchor" href="#arg-iter-level"></a></dt>
<dd><p>Can be <code>"no"</code>, <code>"item"</code> level, <code>"test.att"</code> or <code>"test"</code> level. Default = <code>"no"</code> and
<code>"test.att"</code> can not for <code>"Wald"</code> and <code>"MLR-B"</code>. See details.</p></dd>


<dt id="arg-maxitr">maxitr<a class="anchor" aria-label="anchor" href="#arg-maxitr"></a></dt>
<dd><p>Number of max iterations. Default = <code>1</code>.</p></dd>


<dt id="arg-eps">eps<a class="anchor" aria-label="anchor" href="#arg-eps"></a></dt>
<dd><p>Cut-off points of \(PVAF\), will work when the method is <code>"GDI"</code> or <code>"Wald"</code>.
Default = <code>0.95</code>. When <code>eps = 'logit'</code>, the predicted eps by Najera et al. (2019) will be used. See details.</p></dd>


<dt id="arg-alpha-level">alpha.level<a class="anchor" aria-label="anchor" href="#arg-alpha-level"></a></dt>
<dd><p>alpha level for the wald test. Default = <code>0.05</code> for <code>"Wald"</code> and <code>0.01</code> for <code>"MLR-B"</code></p></dd>


<dt id="arg-criter">criter<a class="anchor" aria-label="anchor" href="#arg-criter"></a></dt>
<dd><p>The kind of fit-index value. When <code>method = "Hull"</code>, it can be <code>R2</code> for
\(R_{McFadden}^2\) @seealso <code><a href="get.R2.html">get.R2</a></code> or <code>'PVAF'</code> for the proportion of
variance accounted for (\(PVAF\)) @seealso <code><a href="get.PVAF.html">get.PVAF</a></code>. When
<code>method = "beta"</code>, it can be <code>'AIC'</code>, <code>'BIC'</code>, <code>'CAIC'</code> or <code>'SABIC'</code>.
Default = <code>"PVAF"</code> for <code>'Hull'</code> and default = <code>"AIC"</code> for <code>'beta'</code>. See details.</p></dd>


<dt id="arg-verbose">verbose<a class="anchor" aria-label="anchor" href="#arg-verbose"></a></dt>
<dd><p>Logical indicating to print iterative information or not. Default is <code>TRUE</code></p></dd>

</dl></div>
    <div id="value">
    <h2>Value</h2>
    <p>An object of class <code>validation</code> containing the following components:</p><dl><dt><code>Q.orig</code></dt>
<dd><p>The original Q-matrix that maybe contain some mis-specifications and need to be validated.</p></dd>

 <dt><code>Q.sug</code></dt>
<dd><p>The Q-matrix that suggested by certain validation method.</p></dd>

 <dt><code>time.cost</code></dt>
<dd><p>The time cost to finish the function.</p></dd>

 <dt><code>process</code></dt>
<dd><p>A matrix that contains the modification process of each question during each iteration.
       Each row represents an iteration, and each column corresponds to the q-vector index of the respective
       question. The order of the indices is consistent with the row numbering in the matrix generated by
       the <code><a href="https://wenchao-ma.github.io/GDINA/reference/attributepattern.html" class="external-link">attributepattern</a></code> function in the <code>GDINA</code> package. Only when
       <code>maxitr</code> &gt; 1, the value is available.</p></dd>

 <dt><code>iter</code></dt>
<dd><p>The number of iteration. Only when <code>maxitr</code> &gt; 1, the value is available.</p></dd>

 <dt><code>priority</code></dt>
<dd><p>An <code>I</code> × <code>K</code> matrix that contains the priority of every attribute for
                each item. Only when the <code>search.method</code> is <code>"PAA"</code>, the value is available. See details.</p></dd>

 <dt><code>Hull.fit</code></dt>
<dd><p>A <code>list</code> containing all the information needed to plot the Hull plot, which is
                available only when <code>method</code> = <code>"Hull"</code>.</p></dd>

 <dt><code>arguments</code></dt>
<dd><p>A list containing all input arguments</p></dd>


</dl></div>
    <div id="the-gdi-method">
    <h2>The GDI method</h2>


<p>The GDI method (de la Torre &amp; Chiu, 2016), as the first Q-matrix validation method
applicable to saturated models, serves as an important foundation for various mainstream
Q-matrix validation methods.</p>
<p>The method calculates the proportion of variance accounted for (\(PVAF\); @seealso <code><a href="get.PVAF.html">get.PVAF</a></code>)
for all possible q-vectors for each item, selects the q-vector with a \(PVAF\) just
greater than the cut-off point (or Epsilon, EPS) as the correction result, and the variance
\(\zeta^2\) is the generalized discriminating index (GDI; de la Torre &amp; Chiu, 2016).
Therefore, the GDI method is also considered as a generalized extension of the \(delta\)
method (de la Torre, 2008), which also takes maximizing discrimination as its basic idea.
In the GDI method, \(\zeta^2\) is defined as the weighted variance of the correct
response probabilities across all mastery patterns, that is:
$$
 \zeta^2 =
 \sum_{l=1}^{2^K} \pi_{l} \left[ P(X_{pi}=1|\boldsymbol{\alpha}_{l}) - \bar{P}_{i} \right]^2
$$
where \(\pi_{l}\) represents the prior probability of mastery pattern \(l\);
\(\bar{P}_{i}=\sum_{k=1}^{K}\pi_{l}P(X_{pi}=1|\boldsymbol{\alpha}_{l})\) is the weighted
average of the correct response probabilities across all attribute mastery patterns.
When the q-vector is correctly specified, the calculated \(\zeta^2\) should be maximized,
indicating the maximum discrimination of the item. However, in reality, \(\zeta^2\)
continues to increase when the q-vector is over-specified, and the more attributes that
are over-specified, the larger \(\zeta^2\) becomes. The q-vector with all attributes set
to 1 (i.e., \(\boldsymbol{q}_{1:K}\)) has the largest \(\zeta^2\) (de la Torre, 2016).
This is because an increase in attributes in the q-vector leads to an increase in item
parameters, resulting in greater differences in correct response probabilities across
attribute patterns and, consequently, increased variance. However, this increase in
variance is spurious. Therefore, de la Torre et al. calculated \(PVAF = \frac{\zeta^2}{\zeta_{1:K}^2}\)
to describe the degree to which the discrimination of the current q-vector explains
the maximum discrimination. They selected an appropriate \(PVAF\) cut-off point to achieve
a balance between q-vector fit and parsimony. According to previous studies,
the \(PVAF\) cut-off point is typically set at 0.95 (Ma &amp; de la Torre, 2020; Najera et al., 2021).
Najera et al. (2019; 2020) proposed using multinomial logistic regression to predict a more
appropriate cut-off point for \(PVAF\).
The cut-off point is denoted as \(eps\), and the predicted regression equation is as follows:</p>
<p>$$
\log \left( \frac{eps}{1-eps} \right)
   = \text{logit}(eps)
   = -0.405 + 2.867 \cdot IQ + 4.840 \times 10^{-4} \cdot N - 3.316 \times 10^{-3} \cdot I
 $$
 Where \(IQ\) represents the item quality, calculated as the average difference between the probability of examinee
 with all attributes answering the item correctly and the probability of an examinee with no attributes answering the question correctly
 (\(IQ = \frac{1}{I} \sum_{i=1}^{I} \left[ P_{i}\left( \boldsymbol{1} \right) - P_{i}\left( \boldsymbol{0} \right)\right]\)),
 and \(N\) and \(I\) represent the number of examinees and the number of questions, respectively.</p>
    </div>
    <div id="the-wald-method">
    <h2>The Wald method</h2>


<p>The Wald method (Ma &amp; de la Torre, 2020) combines the Wald test with \(PVAF\) to correct
the Q-matrix at the item level. Its basic logic is as follows: when correcting item \(i\),
the single attribute that maximizes the \(PVAF\) value is added to a vector with all
attributes set to \(\boldsymbol{0}\) (i.e., \(\boldsymbol{q} = (0, 0, \ldots, 0)\)) as a starting point.
In subsequent iterations, attributes in this vector are continuously added or
removed through the Wald test. The correction process ends when the \(PVAF\) exceeds the
cut-off point or when no further attribute changes occur. The Wald statistic follows an
asymptotic \(\chi^{2}\) distribution with a degree of freedom of \(2^{K^\ast} - 1\).</p>
<p>The calculation method is as follows:
$$
   Wald = \left[\boldsymbol{R} \times \boldsymbol{P}_{i}(\boldsymbol{\alpha})\right]^{'}
   (\boldsymbol{R} \times \boldsymbol{V}_{i} \times \boldsymbol{R})^{-1}
   \left[\boldsymbol{R} \times P_{i}(\boldsymbol{\alpha})\right]
$$
\(\boldsymbol{R}\) represents the restriction matrix (@seealso <code><a href="get.Rmatrix.html">get.Rmatrix</a></code>);
\(\boldsymbol{P}_{i}(\boldsymbol{\alpha})\) denotes
the vector of correct response probabilities for item \(i\); \(\boldsymbol{V}_i\) is the
variance-covariance matrix of the correct response probabilities for item \(i\), which
can be obtained by multiplying the \(\boldsymbol{M}_i\) matrix (de la Torre, 2011) with the
variance-covariance matrix of item parameters \(\boldsymbol{\Sigma}_i\), i.e.,
\(\boldsymbol{V}_i = \boldsymbol{M}_i \times \boldsymbol{\Sigma}_i\). The \(\boldsymbol{\Sigma}_i\) can be
derived by inverting the information matrix. Using the empirical cross-product information
matrix (de la Torre, 2011) to calculate \(\boldsymbol{\Sigma}_i\).</p>
<p>\(\boldsymbol{M}_i\) is a \(2^{K^\ast} \times 2^{K^\ast}\) matrix (@seealso <code><a href="get.Mmatrix.html">get.Mmatrix</a></code>)
that represents the relationship between the parameters of item \(i\) and the attribute mastery patterns. The
rows represent different mastery patterns, while the columns represent different item parameters.</p>
<p>In Package Qval, <a href="https://rdrr.io/r/parallel/clusterApply.html" class="external-link">parLapply</a> will be used to accelerate the Wald method.</p>
    </div>
    <div id="the-hull-method">
    <h2>The Hull method</h2>


<p>The Hull method (Najera et al., 2021) addresses the issue of the cut-off point in the GDI
method and demonstrates good performance in simulation studies. Najera et al. applied the
Hull method for determining the number of factors to retain in exploratory factor analysis
(Lorenzo-Seva et al., 2011) to the retention of attribute quantities in the q-vector, specifically
for Q-matrix validation. The Hull method aligns with the GDI approach in its philosophy
of seeking a balance between fit and parsimony. While GDI relies on a preset, arbitrary
cut-off point to determine this balance, the Hull method utilizes the most pronounced elbow
in the Hull plot to make this judgment. The most pronounced elbow is determined using
the following formula:
$$
   st = \frac{(f_k - f_{k-1}) / (np_k - np_{k-1})}{(f_{k+1} - f_k) / (np_{k+1} - np_k)}
$$
where \(f_k\) represents the fit-index value (can be \(PVAF\) @seealso <code><a href="get.PVAF.html">get.PVAF</a></code> or
\(R2\) @seealso <code><a href="get.R2.html">get.R2</a></code>) when the q-vector contains \(k\) attributes,
similarly, \(f_{k-1}\) and \(f_{k+1}\) represent the fit-index value when the q-vector contains \(k-1\)
and \(k+1\) attributes, respectively. \({np}_k\) denotes the number of parameters when the
q-vector has \(k\) attributes, which is \(2^k\) for a saturated model. Likewise, \({np}_{k-1}\)
and \({np}_{k+1}\) represent the number of parameters when the q-vector has \(k-1\) and
\(k+1\) attributes, respectively. The Hull method calculates the \(st\) index for all possible q-vectors
and retains the q-vector with the maximum \(st\) index as the corrected result.
Najera et al. (2021) removed any concave points from the Hull plot, and when only the first and
last points remained in the plot, the saturated q-vector was selected.</p>
    </div>
    <div id="the-mlr-b-method">
    <h2>The MLR-B method</h2>


<p>The MLR-B method proposed by Tu et al. (2022) differs from the GDI, Wald and Hull method in that
it does not employ \(PVAF\). Instead, it directly uses the marginal probabilities of attribute mastery for
examinees to perform multivariate logistic regression on their observed scores. This approach assumes
all possible q-vectors and conducts \(2^K-1\) regression modelings. After proposing regression equations
that exclude any insignificant regression coefficients, it selects the q-vector corresponding to
the equation with the minimum \(AIC\) value as the validation result. The performance of this method in both the
LCDM and GDM models even surpasses that of the Hull method (Tu et al., 2022), making it an efficient and reliable
approach for Q-matrix validation.</p>
<p>In Package Qval, <a href="https://rdrr.io/r/parallel/clusterApply.html" class="external-link">parLapply</a> will be used to accelerate the MLR-B method when <code>search.method</code> is not <code>"PAA"</code>.</p>
    </div>
    <div id="the-beta-method">
    <h2>The \(\beta\) method</h2>


<p>The \(\beta\) method (Li &amp; Chen, 2024) addresses the Q-matrix validation problem from the
perspective of signal detection theory. Signal detection theory posits that any stimulus is
a signal embedded in noise, where the signal always overlaps with noise. The \(\beta\) method
treats the correct q-vector as the signal and other possible q-vectors as noise. The goal is
to identify the signal from the noise, i.e., to correctly identify the q-vector. For item
\(i\) with the q-vector of the \(c\)-th type, the \(\beta\) index is computed as follows:</p>
<p>$$
   \beta_{ic} = \sum_{l=1}^{2^K} \left| \frac{r_{li}}{n_l} P_{ic}(\boldsymbol{\alpha_l}) -
                \left(1 - \frac{r_{li}}{n_l}\right) \left[1 - P_{ic}(\boldsymbol{\alpha_l})\right] \right|
              = \sum_{l=1}^{2^K} \left| \frac{r_{li}}{n_l} - \left[1 - P_{ic}(\boldsymbol{\alpha_l}) \right] \right|
 $$</p>
<p>In the formula, \(r_{li}\) represents the number of examinees in knowledge state \(l\) who correctly
answered item \(i\), while \(n_l\) is the total number of examinees in knowledge state \(l\).
\(P_{ic}(\boldsymbol{\alpha_l})\) denotes the probability that an examinee in knowledge state \(l\) answers
item \(i\) correctly when the q-vector for item \(i\) is of the \(c\)-th type. In fact,
\(\frac{r_{li}}{n_l}\) is the observed probability that an examinee in knowledge state \(l\) answers
item \(i\) correctly, and \(\beta_{jc}\) represents the difference between the actual proportion of
correct answers for item \(i\) in each knowledge state and the expected probability of answering the
item incorrectly in that state. Therefore, to some extent, \(\beta_{jc}\) can be considered as a measure
of discriminability, and the \(\beta\) method posits that the correct q-vector maximizes \(\beta_{jc}\),
i.e.:</p>
<p>$$
   \boldsymbol{q}_i
   = \arg\max_{\boldsymbol{q}} \left( \beta_{jc} : \boldsymbol{q} \in \left\{ \boldsymbol{q}_{ic},
       \, c = 1, 2, \dots, 2^{K} - 1 \right\} \right)
 $$</p>
<p>Therefore, essentially, \(\beta_{jc}\) is an index similar to GDI. Both increase as the number of attributes
in the q-vector increases. Unlike the GDI method, the \(\beta\) method does not continue to compute
\(\beta_{jc} / \beta_{j[11...1]}\) but instead uses the minimum \(AIC\) value to determine whether the attributes
in the q-vector are sufficient. In Package Qval, <a href="https://rdrr.io/r/parallel/clusterApply.html" class="external-link">parLapply</a> will be used to accelerate the \(\beta\) method.</p>
<p>Please note that the \(\beta\) method has different meanings when applying different search algorithms.
For more details, see section 'Search algorithm' below.</p>
    </div>
    <div id="iterative-procedure">
    <h2>Iterative procedure</h2>


<p>The iterative procedure that one item modification at a time is item level iteration (<code> iter.level = "item"</code>) in (Najera
et al., 2020, 2021). The steps of the <code>item</code> level iterative procedure algorithm are as follows:</p><dl><dt>Step1</dt>
<dd><p>Fit the <code>CDM</code> according to the item responses and the provisional Q-matrix (\(\boldsymbol{Q}^0\)).</p></dd>

   <dt>Step2</dt>
<dd><p>Validate the provisional Q-matrix and gain a suggested Q-matrix (\(\boldsymbol{Q}^1\)).</p></dd>

   <dt>Step3</dt>
<dd><p>for each item, \(PVAF_{0i}\) as the \(PVAF\) of the provisional q-vector specified in \(\boldsymbol{Q}^0\),
               and \(PVAF_{1i}\) as the \(PVAF\) of the suggested q-vector in \(\boldsymbol{Q}^1\).</p></dd>

   <dt>Step4</dt>
<dd><p>Calculate all items' \(\Delta PVAF_{i}\), defined as \(\Delta PVAF_{i} = |PVAF_{1i} - PVAF_{0i}|\)</p></dd>

   <dt>Step5</dt>
<dd><p>Define the hit item as the item with the highest \(\Delta PVAF_{i}\).</p></dd>

   <dt>Step6</dt>
<dd><p>Update \(\boldsymbol{Q}^0\) by changing the provisional q-vector by the suggested q-vector of the hit item.</p></dd>

   <dt>Step7</dt>
<dd><p>Iterate over Steps 1 to 6 until \(\sum_{i=1}^{I} \Delta PVAF_{i} = 0\)</p></dd>


</dl><p>When the Q-matrix validation method is <code>"MLR-B"</code> or <code>"Hull"</code> when <code>criter = "AIC"</code> or <code>criter = "R2"</code>, \(PVAF\) is not used.
In this case, the criterion for determining which item's index will be replaced is \(AIC\) or \(R^2\), respectively.</p>
<p>The iterative procedure that the entire Q-matrix is modified at each iteration
is test level iteration (<code> iter.level = "test"</code>) (Najera et al., 2020; Tu et al., 2022).
The steps of the <code>test</code> level iterative procedure algorithm are as follows:</p><dl><dt>Step1</dt>
<dd><p>Fit the <code>CDM</code> according to the item responses and the provisional Q-matrix (\(\boldsymbol{Q}^0\)).</p></dd>

   <dt>Step2</dt>
<dd><p>Validate the provisional Q-matrix and gain a suggested Q-matrix (\(\boldsymbol{Q}^1\)).</p></dd>

   <dt>Step3</dt>
<dd><p>Check whether \(\boldsymbol{Q}^1 = \boldsymbol{Q}^0\). If <code>TRUE</code>, terminate the iterative algorithm.
             If <code>FALSE</code>, Update \(\boldsymbol{Q}^0\) as \(\boldsymbol{Q}^1\).</p></dd>

   <dt>Step4</dt>
<dd><p>Iterate over Steps 1 and 3 until one of conditions as follows is satisfied: 1. \(\boldsymbol{Q}^1 =
                 \boldsymbol{Q}^0\); 2. Reach the maximum number of iterations (<code>maxitr</code>); 3. \(\boldsymbol{Q}^1\) does not satisfy
                the condition that an attribute is measured by one item at least.</p></dd>


</dl><p><code>iter.level = 'test.att'</code> will use a method called the test-attribute iterative procedure (Najera et al., 2021), which
modifies all items in each iteration while following the principle of minimizing changes in the number of attributes.
Therefore, the test-attribute iterative procedure and the test-level iterative procedure follow the same process for large items.
The key difference is that the test-attribute iterative procedure only allows minimal adjustments to the \(q\)-vector in each iteration.
For example, if the original \(q\)-vector is \([0010]\) and the validation methods suggest \([1110]\),
the test-level iterative procedure can directly update the \(q\)-vector to \([1110]\).
In contrast, the test-attribute iterative procedure can only make a gradual adjustment,
first modifying the \(q\)-vector to either \([1010]\) or \([0110]\).
As a result, the test-attribute iterative procedure is more cautious than the test-level iterative procedure
and may require more iterations.</p>
    </div>
    <div id="search-algorithm">
    <h2>Search algorithm</h2>


<p>Three search algorithms are available: Exhaustive Search Algorithm (ESA), Sequential Search Algorithm (SSA),
and Priority Attribute Algorithm (PAA).
ESA is a brute-force algorithm. When validating the q-vector of a particular item, it traverses all possible
q-vectors and selects the most appropriate one based on the chosen Q-matrix validation method. Since there are
\(2^{K-1}\) possible q-vectors with \(K\) attributes, ESA requires \(2^{K-1}\) searches for each item.</p>
<p>SSA reduces the number of searches by adding one attribute at a time to the q-vector in a stepwise manner.
Therefore, in the worst-case scenario, SSA requires \(K(K-1)/2\) searches.
The detailed steps are as follows:</p><dl><dt>Step 1</dt>
<dd><p>Define an empty q-vector \(\boldsymbol{q}^0=[00...0]\) of length \(K\),
                 where all elements are 0.</p></dd>

   <dt>Step 2</dt>
<dd><p>Examine all single-attribute q-vectors, which are those formed by
                 changing one of the 0s in \(\boldsymbol{q}^0\) to 1.
                 According to the criteria of the chosen Q-matrix validation method,
                 select the optimal single-attribute q-vector, denoted as \(\boldsymbol{q}^1\).</p></dd>

   <dt>Step 3</dt>
<dd><p>Examine all two-attribute q-vectors, which are those formed by changing
                 one of the 0s in \(\boldsymbol{q}^1\) to 1. According to the criteria of the
                 chosen Q-matrix validation method, select the optimal two-attribute q-vector,
                 denoted as \(\boldsymbol{q}^2\).</p></dd>

   <dt>Step 4</dt>
<dd><p>Repeat this process until \(\boldsymbol{q}^K\) is found, or the stopping criterion
                 of the chosen Q-matrix validation method is met.</p></dd>


</dl><p>PAA is a highly efficient and concise algorithm that evaluates whether each attribute needs to be included in the
q-vector based on the priority of the attributes. @seealso <code><a href="get.priority.html">get.priority</a></code>. Therefore, even in
the worst-case scenario, PAA only requires \(K\) searches. The detailed process is as follows:</p><dl><dt>Step 1</dt>
<dd><p>Using the applicable CDM (e.g. the G-DINA model) to estimate the model parameters
                 and obtain the marginal attribute mastery probabilities matrix \(\boldsymbol{\Lambda}\)</p></dd>

   <dt>Step 2</dt>
<dd><p>Use LASSO regression to calculate the priority of each attribute in the q-vector for item \(i\)</p></dd>

   <dt>Step 3</dt>
<dd><p>Check whether each attribute is included in the optimal q-vector based on the attribute
                 priorities from high to low seriatim and output the final suggested q-vector according to the
                 criteria of the chosen Q-matrix validation method.</p></dd>


</dl><p>The calculation of priorities is straightforward (Qin &amp; Guo, 2025): the priority of an attribute is the
regression coefficient obtained from a LASSO multinomial logistic regression, with the attribute
as the independent variable and the response data from the examinees as the dependent variable.
The formula (Tu et al., 2022) is as follows:</p>
<p>$$
 \log[\frac{P(X_{pi} = 1 | \boldsymbol{\Lambda}_{p})}{P(X_{pi} = 0 | \boldsymbol{\Lambda}_{p})}] =
 logit[P(X_{pi} = 1 | \boldsymbol{\Lambda}_{p})] =
 \beta_{i0} + \beta_{i1} \Lambda_{p1} + \ldots + \beta_{ik} \Lambda_{pk} + \ldots + \beta_{iK} \Lambda_{pK}
$$</p>
<p>Where \(X_{pi}\) represents the response of examinee \(p\) on item \(i\),
\(\boldsymbol{\Lambda}_{p}\) denotes the marginal mastery probabilities of examinee \(p\)
(which can be obtained from the return value <code>alpha.P</code> of the <code><a href="CDM.html">CDM</a></code> function),
\(\beta_{i0}\) is the intercept term, and \(\beta_{ik}\) represents the regression coefficient.</p>
<p>The LASSO loss function can be expressed as:</p>
<p>$$l_{lasso}(\boldsymbol{X}_i | \boldsymbol{\Lambda}) = l(\boldsymbol{X}_i | \boldsymbol{\Lambda}) - \lambda |\boldsymbol{\beta}_i|$$</p>
<p>Where \(l_{lasso}(\boldsymbol{X}_i | \boldsymbol{\Lambda})\) is the penalized likelihood,
\(l(\boldsymbol{X}_i | \boldsymbol{\Lambda})\) is the original likelihood,
and \(\lambda\) is the tuning parameter for penalization (a larger value imposes a stronger penalty on
\(\boldsymbol{\beta}_i = [\beta_{i1}, \ldots, \beta_{ik}, \ldots, \beta_{iK}]\)).
The priority for attribute \(i\) is defined as: \(\boldsymbol{priority}_i = \boldsymbol{\beta}_i = [\beta_{i1}, \ldots, \beta_{ik}, \ldots, \beta_{iK}]\)</p>
<p>It should be noted that the Wald method proposed by Ma and de la Torre (2020) uses a <code>"stepwise"</code> search approach.
This approach involves incrementally adding or removing 1 from the q-vector and evaluating the significance of
the change using the Wald test:
1. If removing a 1 results in non-significance (indicating that the 1 is unnecessary), the 1 is removed from the q-vector;
   otherwise, the q-vector remains unchanged.
2. If adding a 1 results in significance (indicating that the 1 is necessary), the 1 is added to the q-vector;
   otherwise, the q-vector remains unchanged.
The process stops when the q-vector no longer changes or when the PVAF reaches the preset cut-off point (i.e., 0.95).
Stepwise are unique search approach of the Wald method, and users should be aware of this. Since stepwise is
inefficient and differs significantly from the extremely high efficiency of PAA, <code>Qval</code> package also provides <code>PAA</code>
for q-vector search in the Wald method. When applying the PAA version of the Wald method, the search still
examines whether each attribute is necessary (by checking if the Wald test reaches significance after adding the attribute)
according to attribute priority. The search stops when no further necessary attributes are found or when the
PVAF reaches the preset cut-off point (i.e., 0.95). The "forward" search approach is another search method
available for the Wald method, which is equivalent to <code>"SSA"</code>. When <code>"Wald"</code> uses <code>search.method = "SSA"</code>,
it means that the Wald method is employing the forward search approach. Its basic process is the same as <code>'stepwise'</code>,
except that it does not remove elements from the q-vector. Therefore, the "forward" search approach is essentially equivalent to SSA.</p>
<p>Please note that, since the \(\beta\) method essentially selects q-vectors based on \(AIC\), even without using the iterative process,
the \(\beta\) method requires multiple parameter estimations to obtain the AIC values for different q-vectors.
Therefore, the \(\beta\) method is more time-consuming and computationally intensive compared to the other methods.
Li and Chen (2024) introduced a specialized search approach for the \(\beta\) method, which is referred to as the
\(\beta\) search (<code>search.method = 'beta'</code>). The number of searches required is \(2^{K-2} + K + 1\), and
the specific steps are as follows:</p><dl><dt>Step 1</dt>
<dd><p>For item \(i\), sequentially examine the \(\beta\) values for each single-attribute q-vector,
                 select the largest \(\beta_{most}\) and the smallest \(\beta_{least}\), along with the corresponding
                 attributes \(k_{most}\) and \(k_{least}\). (K searches)</p></dd>

   <dt>Step 2</dt>
<dd><p>Then, add all possible q-vectors (a total of \(2^K - 1\)) containing attribute \(k_{most}\) and
                 not containing \(k_{least}\) to the search space \(\boldsymbol{S}_i\) (a total of \(2^{K-2}\))), and unconditionally
                 add the saturated q-vector \([11\ldots1]\) to \(\boldsymbol{S}_i\) to ensure that it is tested.</p></dd>

   <dt>Step 3</dt>
<dd><p>Select the q-vector with the minimum AIC from \(\boldsymbol{S}_i\) as the final output of the \(\beta\)
                 method. (The remaining \(2^{K-2} + 1\) searches)</p></dd>


</dl><p>The <code>Qval</code> package also provides three search methods, ESA, SSA, and PAA, for the \(\beta\) method.
When the \(\beta\) method applies these three search methods, Q-matrix validation can be completed without
calculating any \(\beta\) values, as the \(\beta\) method essentially uses <code>AIC</code> for selecting q-vectors.
For example, when applying ESA, the \(\beta\) method does not need to perform Step 1 of the \(\beta\) search
and only needs to include all possible q-vectors (a total of \(2^K - 1\)) in \(\boldsymbol{S}_i\), then outputs
the corresponding q-vector based on the minimum \(AIC\). When applying SSA or PAA, the \(\beta\) method also
does not require any calculation of \(\beta\) values. In this case, the \(\beta\) method is consistent
with the Q-matrix validation process described by Chen et al. (2013) using relative fit indices. Therefore, when
the \(\beta\) method does not use \(\beta\) search, it is equivalent to the method of Chen et al. (2013).
To better implement Chen et al. (2013)'s Q-matrix validation method using relative fit indices, the <code>Qval</code>
package also provides \(BIC\), \(CAIC\), and \(SABIC\) as alternatives to validate q-vectors, in addition
to \(AIC\).</p>
    </div>
    <div id="references">
    <h2>References</h2>
    <p>Chen, J., de la Torre, J., &amp; Zhang, Z. (2013). Relative and Absolute Fit Evaluation in Cognitive Diagnosis Modeling. Journal of Educational Measurement, 50(2), 123-140. DOI: 10.1111/j.1745-3984.2012.00185.x</p>
<p>de la Torre, J., &amp; Chiu, C. Y. (2016). A General Method of Empirical Q-matrix Validation. Psychometrika, 81(2), 253-273. DOI: 10.1007/s11336-015-9467-8.</p>
<p>de la Torre, J. (2008). An Empirically Based Method of Q-Matrix Validation for the DINA Model: Development and Applications. Journal of Education Measurement, 45(4), 343-362. DOI: 10.1111/j.1745-3984.2008.00069.x.</p>
<p>Li, J., &amp; Chen, P. (2024). A new Q-matrix validation method based on signal detection theory. British Journal of Mathematical and Statistical Psychology, 00, 1–33. DOI: 10.1111/bmsp.12371</p>
<p>Lorenzo-Seva, U., Timmerman, M. E., &amp; Kiers, H. A. (2011). The Hull method for selecting the number of common factors. Multivariate Behavioral Research, 46, 340–364. DOI: 10.1080/00273171.2011.564527.</p>
<p>Ma, W., &amp; de la Torre, J. (2020). An empirical Q-matrix validation method for the sequential generalized DINA model. British Journal of Mathematical and Statistical Psychology, 73(1), 142-163. DOI: 10.1111/bmsp.12156.</p>
<p>McFadden, D. (1974). Conditional logit analysis of qualitative choice behavior. In P. Zarembka (Ed.), Frontiers in economics (pp. 105–142). New York, NY: Academic Press.</p>
<p>Najera, P., Sorrel, M. A., &amp; Abad, F. J. (2019). Reconsidering Cutoff Points in the General Method of Empirical Q-Matrix Validation. Educational and Psychological Measurement, 79(4), 727-753. DOI: 10.1177/0013164418822700.</p>
<p>Najera, P., Sorrel, M. A., de la Torre, J., &amp; Abad, F. J. (2020). Improving Robustness in Q-Matrix Validation Using an Iterative and Dynamic Procedure. Applied Psychological Measurement, 44(6), 431-446. DOI: 10.1177/0146621620909904.</p>
<p>Najera, P., Sorrel, M. A., de la Torre, J., &amp; Abad, F. J. (2021). Balancing fit and parsimony to improve Q-matrix validation. British Journal of Mathematical and Statistical Psychology, 74 Suppl 1, 110-130. DOI: 10.1111/bmsp.12228.</p>
<p>Qin, H., &amp; Guo, L. (2025). Priority attribute algorithm for Q-matrix validation: A didactic. Behavior Research Methods, 57(1), 31. DOI: 10.3758/s13428-024-02547-5.</p>
<p>Terzi, R., &amp; de la Torre, J. (2018). An Iterative Method for Empirically-Based Q-Matrix Validation. International Journal of Assessment Tools in Education, 248-262. DOI: 10.21449/ijate.40719.</p>
<p>Tu, D., Chiu, J., Ma, W., Wang, D., Cai, Y., &amp; Ouyang, X. (2022). A multiple logistic regression-based (MLR-B) Q-matrix validation method for cognitive diagnosis models: A confirmatory approach. Behavior Research Methods. DOI: 10.3758/s13428-022-01880-x.</p>
    </div>
    <div id="author">
    <h2>Author</h2>
    <p>Haijiang Qin &lt;Haijiang133@outlook.com&gt;</p>
    </div>

    <div id="ref-examples">
    <h2>Examples</h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="co">################################################################</span></span></span>
<span class="r-in"><span><span class="co">#                           Example 1                          #</span></span></span>
<span class="r-in"><span><span class="co">#             The GDI method to validate Q-matrix              #</span></span></span>
<span class="r-in"><span><span class="co">################################################################</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://haijiangqin.com/Qval/" class="external-link">Qval</a></span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## generate Q-matrix and data</span></span></span>
<span class="r-in"><span><span class="va">K</span> <span class="op">&lt;-</span> <span class="fl">3</span></span></span>
<span class="r-in"><span><span class="va">I</span> <span class="op">&lt;-</span> <span class="fl">20</span></span></span>
<span class="r-in"><span><span class="va">Q</span> <span class="op">&lt;-</span> <span class="fu"><a href="sim.Q.html">sim.Q</a></span><span class="op">(</span><span class="va">K</span>, <span class="va">I</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">IQ</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  P0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="va">I</span>, <span class="fl">0.0</span>, <span class="fl">0.2</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>  P1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="va">I</span>, <span class="fl">0.8</span>, <span class="fl">1.0</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="sim.data.html">sim.data</a></span><span class="op">(</span>Q <span class="op">=</span> <span class="va">Q</span>, N <span class="op">=</span> <span class="fl">500</span>, IQ <span class="op">=</span> <span class="va">IQ</span>,</span></span>
<span class="r-in"><span>                         model <span class="op">=</span> <span class="st">"GDINA"</span>, distribute <span class="op">=</span> <span class="st">"horder"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> distribute =  horder </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> model =  GDINA </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  number of attributes:  3 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  number of items:  20 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  num of examinees:  500 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  average of P0 =  0.083 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  average of P1 =  0.894 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> theta_mean =  -0.055 , theta_sd = 0.996 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  a =  1.5 1.5 1.5 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  b =  -1.5 1.5 0 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## simulate random mis-specifications</span></span></span>
<span class="r-in"><span><span class="va">MQ</span> <span class="op">&lt;-</span> <span class="fu"><a href="sim.MQ.html">sim.MQ</a></span><span class="op">(</span><span class="va">Q</span>, <span class="fl">0.2</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> rate of mis-specifications =  0.2 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  rate of  over-specifications =  0.13 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  rate of under-specifications =  0.07 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## using MMLE/EM to fit CDM model first</span></span></span>
<span class="r-in"><span><span class="va">CDM.obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="CDM.html">CDM</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter = 1  Max. abs. change = 0.64296  Deviance  = 11950.87                                                                                  Iter = 2  Max. abs. change = 0.32047  Deviance  = 9349.31                                                                                  Iter = 3  Max. abs. change = 0.28192  Deviance  = 9112.87                                                                                  Iter = 4  Max. abs. change = 0.14554  Deviance  = 9043.82                                                                                  Iter = 5  Max. abs. change = 0.07032  Deviance  = 8995.02                                                                                  Iter = 6  Max. abs. change = 0.06698  Deviance  = 8960.85                                                                                  Iter = 7  Max. abs. change = 0.05455  Deviance  = 8939.01                                                                                  Iter = 8  Max. abs. change = 0.03934  Deviance  = 8926.79                                                                                  Iter = 9  Max. abs. change = 0.04037  Deviance  = 8919.34                                                                                  Iter = 10  Max. abs. change = 0.03906  Deviance  = 8914.39                                                                                  Iter = 11  Max. abs. change = 0.03598  Deviance  = 8911.42                                                                                  Iter = 12  Max. abs. change = 0.04102  Deviance  = 8909.46                                                                                  Iter = 13  Max. abs. change = 0.04857  Deviance  = 8908.02                                                                                  Iter = 14  Max. abs. change = 0.06070  Deviance  = 8906.82                                                                                  Iter = 15  Max. abs. change = 0.07878  Deviance  = 8905.64                                                                                  Iter = 16  Max. abs. change = 0.09908  Deviance  = 8904.33                                                                                  Iter = 17  Max. abs. change = 0.10262  Deviance  = 8902.84                                                                                  Iter = 18  Max. abs. change = 0.07095  Deviance  = 8901.44                                                                                  Iter = 19  Max. abs. change = 0.03197  Deviance  = 8900.50                                                                                  Iter = 20  Max. abs. change = 0.02242  Deviance  = 8900.00                                                                                  Iter = 21  Max. abs. change = 0.01533  Deviance  = 8899.75                                                                                  Iter = 22  Max. abs. change = 0.01051  Deviance  = 8899.61                                                                                  Iter = 23  Max. abs. change = 0.00734  Deviance  = 8899.53                                                                                  Iter = 24  Max. abs. change = 0.00524  Deviance  = 8899.47                                                                                  Iter = 25  Max. abs. change = 0.00384  Deviance  = 8899.42                                                                                  Iter = 26  Max. abs. change = 0.00289  Deviance  = 8899.38                                                                                  Iter = 27  Max. abs. change = 0.00223  Deviance  = 8899.35                                                                                  Iter = 28  Max. abs. change = 0.00179  Deviance  = 8899.33                                                                                  Iter = 29  Max. abs. change = 0.00166  Deviance  = 8899.30                                                                                  Iter = 30  Max. abs. change = 0.00155  Deviance  = 8899.28                                                                                  Iter = 31  Max. abs. change = 0.00144  Deviance  = 8899.27                                                                                  Iter = 32  Max. abs. change = 0.00134  Deviance  = 8899.25                                                                                  Iter = 33  Max. abs. change = 0.00124  Deviance  = 8899.24                                                                                  Iter = 34  Max. abs. change = 0.00115  Deviance  = 8899.22                                                                                  Iter = 35  Max. abs. change = 0.00106  Deviance  = 8899.21                                                                                  Iter = 36  Max. abs. change = 0.00098  Deviance  = 8899.20                                                                                  Iter = 37  Max. abs. change = 0.00090  Deviance  = 8899.20                                                                                  Iter = 38  Max. abs. change = 0.00083  Deviance  = 8899.19                                                                                  Iter = 39  Max. abs. change = 0.00076  Deviance  = 8899.18                                                                                  Iter = 40  Max. abs. change = 0.00069  Deviance  = 8899.18                                                                                  Iter = 41  Max. abs. change = 0.00063  Deviance  = 8899.17                                                                                  Iter = 42  Max. abs. change = 0.00057  Deviance  = 8899.17                                                                                  Iter = 43  Max. abs. change = 0.00051  Deviance  = 8899.16                                                                                  Iter = 44  Max. abs. change = 0.00045  Deviance  = 8899.16                                                                                  Iter = 45  Max. abs. change = 0.00040  Deviance  = 8899.16                                                                                  Iter = 46  Max. abs. change = 0.00035  Deviance  = 8899.16                                                                                  Iter = 47  Max. abs. change = 0.00032  Deviance  = 8899.16                                                                                  Iter = 48  Max. abs. change = 0.00028  Deviance  = 8899.16                                                                                  Iter = 49  Max. abs. change = 0.00025  Deviance  = 8899.16                                                                                  Iter = 50  Max. abs. change = 0.00022  Deviance  = 8899.15                                                                                  Iter = 51  Max. abs. change = 0.00020  Deviance  = 8899.15                                                                                  Iter = 52  Max. abs. change = 0.00018  Deviance  = 8899.15                                                                                  Iter = 53  Max. abs. change = 0.00016  Deviance  = 8899.15                                                                                  Iter = 54  Max. abs. change = 0.00014  Deviance  = 8899.15                                                                                  Iter = 55  Max. abs. change = 0.00013  Deviance  = 8899.15                                                                                  Iter = 56  Max. abs. change = 0.00012  Deviance  = 8899.15                                                                                  Iter = 57  Max. abs. change = 0.00011  Deviance  = 8899.15                                                                                  Iter = 58  Max. abs. change = 0.00010  Deviance  = 8899.15                                                                                  </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## using the fitted CDM.obj to avoid extra parameter estimation.</span></span></span>
<span class="r-in"><span><span class="va">Q.GDI.obj</span> <span class="op">&lt;-</span> <span class="fu">validation</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span>, CDM.obj<span class="op">=</span><span class="va">CDM.obj</span>, method <span class="op">=</span> <span class="st">"GDI"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> GDI  method with  PAA  in  test  level iteration ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  1/  1,  12 items have changed, ΔPVAF=1.78720 </span>
<span class="r-in"><span><span class="co">## check QRR</span></span></span>
<span class="r-in"><span><span class="fu"><a href="print.html">print</a></span><span class="op">(</span><span class="fu"><a href="zQRR.html">zQRR</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">Q.GDI.obj</span><span class="op">$</span><span class="va">Q.sug</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] 0.95</span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># \donttest{</span></span></span>
<span class="r-in"><span><span class="co">## also can validate the Q-matrix directly</span></span></span>
<span class="r-in"><span><span class="va">Q.GDI.obj</span> <span class="op">&lt;-</span> <span class="fu">validation</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> GDI  method with  PAA  in  test  level iteration ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  1/  1,  12 items have changed, ΔPVAF=1.78720 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## item level iteration</span></span></span>
<span class="r-in"><span><span class="va">Q.GDI.obj</span> <span class="op">&lt;-</span> <span class="fu">validation</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span>, method <span class="op">=</span> <span class="st">"GDI"</span>,</span></span>
<span class="r-in"><span>                        iter.level <span class="op">=</span> <span class="st">"item"</span>, maxitr <span class="op">=</span> <span class="fl">150</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> GDI  method with  PAA  in  item  level iteration ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  1/150,   1 items have changed, ΔPVAF=0.90166 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  2/150,   1 items have changed, ΔPVAF=0.52635 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  3/150,   1 items have changed, ΔPVAF=0.29593 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  4/150,   1 items have changed, ΔPVAF=0.03496 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  5/150,   1 items have changed, ΔPVAF=0.03932 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  6/150,   1 items have changed, ΔPVAF=0.03920 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  7/150,   1 items have changed, ΔPVAF=0.00989 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  8/150,   1 items have changed, ΔPVAF=0.00306 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  9/150,   1 items have changed, ΔPVAF=0.00178 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  = 10/150,   1 items have changed, ΔPVAF=0.00319 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  = 11/150,   1 items have changed, ΔPVAF=0.00134 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  = 12/150,   1 items have changed, ΔPVAF=0.00065 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  = 13/150,   1 items have changed, ΔPVAF=0.00011 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## search method</span></span></span>
<span class="r-in"><span><span class="va">Q.GDI.obj</span> <span class="op">&lt;-</span> <span class="fu">validation</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span>, method <span class="op">=</span> <span class="st">"GDI"</span>,</span></span>
<span class="r-in"><span>                        search.method <span class="op">=</span> <span class="st">"ESA"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> GDI  method with  ESA  in  test  level iteration ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  1/  1,  12 items have changed, ΔPVAF=1.78720 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## cut-off point</span></span></span>
<span class="r-in"><span><span class="va">Q.GDI.obj</span> <span class="op">&lt;-</span> <span class="fu">validation</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span>, method <span class="op">=</span> <span class="st">"GDI"</span>,</span></span>
<span class="r-in"><span>                        eps <span class="op">=</span> <span class="fl">0.90</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> GDI  method with  PAA  in  test  level iteration ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =  1/  1,  15 items have changed, ΔPVAF=1.97686 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## check QRR</span></span></span>
<span class="r-in"><span><span class="fu"><a href="print.html">print</a></span><span class="op">(</span><span class="fu"><a href="zQRR.html">zQRR</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">Q.GDI.obj</span><span class="op">$</span><span class="va">Q.sug</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] 0.9</span>
<span class="r-in"><span><span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">################################################################</span></span></span>
<span class="r-in"><span><span class="co">#                           Example 2                          #</span></span></span>
<span class="r-in"><span><span class="co">#             The Hull method to validate Q-matrix             #</span></span></span>
<span class="r-in"><span><span class="co">################################################################</span></span></span>
<span class="r-in"><span><span class="co"># \donttest{</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://haijiangqin.com/Qval/" class="external-link">Qval</a></span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## generate Q-matrix and data</span></span></span>
<span class="r-in"><span><span class="va">K</span> <span class="op">&lt;-</span> <span class="fl">4</span></span></span>
<span class="r-in"><span><span class="va">I</span> <span class="op">&lt;-</span> <span class="fl">20</span></span></span>
<span class="r-in"><span><span class="va">Q</span> <span class="op">&lt;-</span> <span class="fu"><a href="sim.Q.html">sim.Q</a></span><span class="op">(</span><span class="va">K</span>, <span class="va">I</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">IQ</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  P0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="va">I</span>, <span class="fl">0.0</span>, <span class="fl">0.2</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>  P1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="va">I</span>, <span class="fl">0.8</span>, <span class="fl">1.0</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="sim.data.html">sim.data</a></span><span class="op">(</span>Q <span class="op">=</span> <span class="va">Q</span>, N <span class="op">=</span> <span class="fl">500</span>, IQ <span class="op">=</span> <span class="va">IQ</span>, model <span class="op">=</span> <span class="st">"GDINA"</span>,</span></span>
<span class="r-in"><span>                         distribute <span class="op">=</span> <span class="st">"horder"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> distribute =  horder </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> model =  GDINA </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  number of attributes:  4 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  number of items:  20 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  num of examinees:  500 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  average of P0 =  0.085 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  average of P1 =  0.898 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> theta_mean =  -0.06 , theta_sd = 0.996 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  a =  1.5 1.5 1.5 1.5 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  b =  0.5 1.5 -1.5 -0.5 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## simulate random mis-specifications</span></span></span>
<span class="r-in"><span><span class="va">MQ</span> <span class="op">&lt;-</span> <span class="fu"><a href="sim.MQ.html">sim.MQ</a></span><span class="op">(</span><span class="va">Q</span>, <span class="fl">0.1</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> rate of mis-specifications =  0.1 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  rate of  over-specifications =  0.01 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  rate of under-specifications =  0.09 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## using MMLE/EM to fit CDM first</span></span></span>
<span class="r-in"><span><span class="va">CDM.obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="CDM.html">CDM</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter = 1  Max. abs. change = 0.48443  Deviance  = 11093.48                                                                                  Iter = 2  Max. abs. change = 0.11245  Deviance  = 9689.83                                                                                  Iter = 3  Max. abs. change = 0.09617  Deviance  = 9638.75                                                                                  Iter = 4  Max. abs. change = 0.05556  Deviance  = 9624.85                                                                                  Iter = 5  Max. abs. change = 0.03311  Deviance  = 9617.39                                                                                  Iter = 6  Max. abs. change = 0.02636  Deviance  = 9612.61                                                                                  Iter = 7  Max. abs. change = 0.02569  Deviance  = 9609.41                                                                                  Iter = 8  Max. abs. change = 0.02618  Deviance  = 9607.29                                                                                  Iter = 9  Max. abs. change = 0.02528  Deviance  = 9605.94                                                                                  Iter = 10  Max. abs. change = 0.02268  Deviance  = 9605.12                                                                                  Iter = 11  Max. abs. change = 0.01895  Deviance  = 9604.64                                                                                  Iter = 12  Max. abs. change = 0.01489  Deviance  = 9604.35                                                                                  Iter = 13  Max. abs. change = 0.01110  Deviance  = 9604.18                                                                                  Iter = 14  Max. abs. change = 0.00794  Deviance  = 9604.08                                                                                  Iter = 15  Max. abs. change = 0.00548  Deviance  = 9604.02                                                                                  Iter = 16  Max. abs. change = 0.00369  Deviance  = 9603.98                                                                                  Iter = 17  Max. abs. change = 0.00243  Deviance  = 9603.96                                                                                  Iter = 18  Max. abs. change = 0.00158  Deviance  = 9603.94                                                                                  Iter = 19  Max. abs. change = 0.00118  Deviance  = 9603.94                                                                                  Iter = 20  Max. abs. change = 0.00112  Deviance  = 9603.93                                                                                  Iter = 21  Max. abs. change = 0.00106  Deviance  = 9603.93                                                                                  Iter = 22  Max. abs. change = 0.00101  Deviance  = 9603.92                                                                                  Iter = 23  Max. abs. change = 0.00095  Deviance  = 9603.92                                                                                  Iter = 24  Max. abs. change = 0.00090  Deviance  = 9603.92                                                                                  Iter = 25  Max. abs. change = 0.00085  Deviance  = 9603.92                                                                                  Iter = 26  Max. abs. change = 0.00080  Deviance  = 9603.92                                                                                  Iter = 27  Max. abs. change = 0.00076  Deviance  = 9603.92                                                                                  Iter = 28  Max. abs. change = 0.00071  Deviance  = 9603.92                                                                                  Iter = 29  Max. abs. change = 0.00067  Deviance  = 9603.92                                                                                  Iter = 30  Max. abs. change = 0.00064  Deviance  = 9603.92                                                                                  Iter = 31  Max. abs. change = 0.00060  Deviance  = 9603.92                                                                                  Iter = 32  Max. abs. change = 0.00057  Deviance  = 9603.92                                                                                  Iter = 33  Max. abs. change = 0.00053  Deviance  = 9603.92                                                                                  Iter = 34  Max. abs. change = 0.00050  Deviance  = 9603.92                                                                                  Iter = 35  Max. abs. change = 0.00048  Deviance  = 9603.92                                                                                  Iter = 36  Max. abs. change = 0.00045  Deviance  = 9603.92                                                                                  Iter = 37  Max. abs. change = 0.00043  Deviance  = 9603.92                                                                                  Iter = 38  Max. abs. change = 0.00040  Deviance  = 9603.92                                                                                  Iter = 39  Max. abs. change = 0.00038  Deviance  = 9603.92                                                                                  Iter = 40  Max. abs. change = 0.00036  Deviance  = 9603.92                                                                                  Iter = 41  Max. abs. change = 0.00034  Deviance  = 9603.92                                                                                  Iter = 42  Max. abs. change = 0.00032  Deviance  = 9603.92                                                                                  Iter = 43  Max. abs. change = 0.00031  Deviance  = 9603.92                                                                                  Iter = 44  Max. abs. change = 0.00029  Deviance  = 9603.92                                                                                  Iter = 45  Max. abs. change = 0.00028  Deviance  = 9603.92                                                                                  Iter = 46  Max. abs. change = 0.00026  Deviance  = 9603.92                                                                                  Iter = 47  Max. abs. change = 0.00025  Deviance  = 9603.92                                                                                  Iter = 48  Max. abs. change = 0.00024  Deviance  = 9603.92                                                                                  Iter = 49  Max. abs. change = 0.00022  Deviance  = 9603.92                                                                                  Iter = 50  Max. abs. change = 0.00021  Deviance  = 9603.92                                                                                  Iter = 51  Max. abs. change = 0.00020  Deviance  = 9603.92                                                                                  Iter = 52  Max. abs. change = 0.00019  Deviance  = 9603.92                                                                                  Iter = 53  Max. abs. change = 0.00018  Deviance  = 9603.92                                                                                  Iter = 54  Max. abs. change = 0.00017  Deviance  = 9603.92                                                                                  Iter = 55  Max. abs. change = 0.00017  Deviance  = 9603.92                                                                                  Iter = 56  Max. abs. change = 0.00016  Deviance  = 9603.92                                                                                  Iter = 57  Max. abs. change = 0.00015  Deviance  = 9603.92                                                                                  Iter = 58  Max. abs. change = 0.00014  Deviance  = 9603.92                                                                                  Iter = 59  Max. abs. change = 0.00014  Deviance  = 9603.92                                                                                  Iter = 60  Max. abs. change = 0.00013  Deviance  = 9603.92                                                                                  Iter = 61  Max. abs. change = 0.00012  Deviance  = 9603.92                                                                                  Iter = 62  Max. abs. change = 0.00012  Deviance  = 9603.92                                                                                  Iter = 63  Max. abs. change = 0.00011  Deviance  = 9603.92                                                                                  Iter = 64  Max. abs. change = 0.00011  Deviance  = 9603.92                                                                                  Iter = 65  Max. abs. change = 0.00010  Deviance  = 9603.92                                                                                  Iter = 66  Max. abs. change = 0.00010  Deviance  = 9603.92                                                                                  </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## using the fitted CDM.obj to avoid extra parameter estimation.</span></span></span>
<span class="r-in"><span><span class="va">Q.Hull.obj</span> <span class="op">&lt;-</span> <span class="fu">validation</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span>, <span class="va">CDM.obj</span>, method <span class="op">=</span> <span class="st">"Hull"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Hull  method with  PAA  in  test  level iteration ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =   1/   1, 9 items have changed, ΔPVAF=0.86208 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## also can validate the Q-matrix directly</span></span></span>
<span class="r-in"><span><span class="va">Q.Hull.obj</span> <span class="op">&lt;-</span> <span class="fu">validation</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span>, method <span class="op">=</span> <span class="st">"Hull"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Hull  method with  PAA  in  test  level iteration ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =   1/   1, 9 items have changed, ΔPVAF=0.86208 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## change PVAF to R2 as fit-index</span></span></span>
<span class="r-in"><span><span class="va">Q.Hull.obj</span> <span class="op">&lt;-</span> <span class="fu">validation</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span>, method <span class="op">=</span> <span class="st">"Hull"</span>, criter <span class="op">=</span> <span class="st">"R2"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Hull  method with  PAA  in  test  level iteration ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =   1/   1, 9 items have changed, ΔR2=0.36480 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## check QRR</span></span></span>
<span class="r-in"><span><span class="fu"><a href="print.html">print</a></span><span class="op">(</span><span class="fu"><a href="zQRR.html">zQRR</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">Q.Hull.obj</span><span class="op">$</span><span class="va">Q.sug</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] 0.9375</span>
<span class="r-in"><span><span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">################################################################</span></span></span>
<span class="r-in"><span><span class="co">#                           Example 3                          #</span></span></span>
<span class="r-in"><span><span class="co">#             The MLR-B method to validate Q-matrix            #</span></span></span>
<span class="r-in"><span><span class="co">################################################################</span></span></span>
<span class="r-in"><span><span class="co"># \donttest{</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://haijiangqin.com/Qval/" class="external-link">Qval</a></span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## generate Q-matrix and data</span></span></span>
<span class="r-in"><span><span class="va">K</span> <span class="op">&lt;-</span> <span class="fl">4</span></span></span>
<span class="r-in"><span><span class="va">I</span> <span class="op">&lt;-</span> <span class="fl">20</span></span></span>
<span class="r-in"><span><span class="va">Q</span> <span class="op">&lt;-</span> <span class="fu"><a href="sim.Q.html">sim.Q</a></span><span class="op">(</span><span class="va">K</span>, <span class="va">I</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">IQ</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  P0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="va">I</span>, <span class="fl">0.0</span>, <span class="fl">0.2</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>  P1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="va">I</span>, <span class="fl">0.8</span>, <span class="fl">1.0</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="sim.data.html">sim.data</a></span><span class="op">(</span>Q <span class="op">=</span> <span class="va">Q</span>, N <span class="op">=</span> <span class="fl">500</span>, IQ <span class="op">=</span> <span class="va">IQ</span>, model <span class="op">=</span> <span class="st">"GDINA"</span>,</span></span>
<span class="r-in"><span>                         distribute <span class="op">=</span> <span class="st">"horder"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> distribute =  horder </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> model =  GDINA </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  number of attributes:  4 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  number of items:  20 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  num of examinees:  500 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  average of P0 =  0.085 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  average of P1 =  0.898 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> theta_mean =  -0.06 , theta_sd = 0.996 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  a =  1.5 1.5 1.5 1.5 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  b =  0.5 1.5 -1.5 -0.5 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## simulate random mis-specifications</span></span></span>
<span class="r-in"><span><span class="va">MQ</span> <span class="op">&lt;-</span> <span class="fu"><a href="sim.MQ.html">sim.MQ</a></span><span class="op">(</span><span class="va">Q</span>, <span class="fl">0.1</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> rate of mis-specifications =  0.1 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  rate of  over-specifications =  0.01 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  rate of under-specifications =  0.09 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## using MMLE/EM to fit CDM first</span></span></span>
<span class="r-in"><span><span class="va">CDM.obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="CDM.html">CDM</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter = 1  Max. abs. change = 0.48443  Deviance  = 11093.48                                                                                  Iter = 2  Max. abs. change = 0.11245  Deviance  = 9689.83                                                                                  Iter = 3  Max. abs. change = 0.09617  Deviance  = 9638.75                                                                                  Iter = 4  Max. abs. change = 0.05556  Deviance  = 9624.85                                                                                  Iter = 5  Max. abs. change = 0.03311  Deviance  = 9617.39                                                                                  Iter = 6  Max. abs. change = 0.02636  Deviance  = 9612.61                                                                                  Iter = 7  Max. abs. change = 0.02569  Deviance  = 9609.41                                                                                  Iter = 8  Max. abs. change = 0.02618  Deviance  = 9607.29                                                                                  Iter = 9  Max. abs. change = 0.02528  Deviance  = 9605.94                                                                                  Iter = 10  Max. abs. change = 0.02268  Deviance  = 9605.12                                                                                  Iter = 11  Max. abs. change = 0.01895  Deviance  = 9604.64                                                                                  Iter = 12  Max. abs. change = 0.01489  Deviance  = 9604.35                                                                                  Iter = 13  Max. abs. change = 0.01110  Deviance  = 9604.18                                                                                  Iter = 14  Max. abs. change = 0.00794  Deviance  = 9604.08                                                                                  Iter = 15  Max. abs. change = 0.00548  Deviance  = 9604.02                                                                                  Iter = 16  Max. abs. change = 0.00369  Deviance  = 9603.98                                                                                  Iter = 17  Max. abs. change = 0.00243  Deviance  = 9603.96                                                                                  Iter = 18  Max. abs. change = 0.00158  Deviance  = 9603.94                                                                                  Iter = 19  Max. abs. change = 0.00118  Deviance  = 9603.94                                                                                  Iter = 20  Max. abs. change = 0.00112  Deviance  = 9603.93                                                                                  Iter = 21  Max. abs. change = 0.00106  Deviance  = 9603.93                                                                                  Iter = 22  Max. abs. change = 0.00101  Deviance  = 9603.92                                                                                  Iter = 23  Max. abs. change = 0.00095  Deviance  = 9603.92                                                                                  Iter = 24  Max. abs. change = 0.00090  Deviance  = 9603.92                                                                                  Iter = 25  Max. abs. change = 0.00085  Deviance  = 9603.92                                                                                  Iter = 26  Max. abs. change = 0.00080  Deviance  = 9603.92                                                                                  Iter = 27  Max. abs. change = 0.00076  Deviance  = 9603.92                                                                                  Iter = 28  Max. abs. change = 0.00071  Deviance  = 9603.92                                                                                  Iter = 29  Max. abs. change = 0.00067  Deviance  = 9603.92                                                                                  Iter = 30  Max. abs. change = 0.00064  Deviance  = 9603.92                                                                                  Iter = 31  Max. abs. change = 0.00060  Deviance  = 9603.92                                                                                  Iter = 32  Max. abs. change = 0.00057  Deviance  = 9603.92                                                                                  Iter = 33  Max. abs. change = 0.00053  Deviance  = 9603.92                                                                                  Iter = 34  Max. abs. change = 0.00050  Deviance  = 9603.92                                                                                  Iter = 35  Max. abs. change = 0.00048  Deviance  = 9603.92                                                                                  Iter = 36  Max. abs. change = 0.00045  Deviance  = 9603.92                                                                                  Iter = 37  Max. abs. change = 0.00043  Deviance  = 9603.92                                                                                  Iter = 38  Max. abs. change = 0.00040  Deviance  = 9603.92                                                                                  Iter = 39  Max. abs. change = 0.00038  Deviance  = 9603.92                                                                                  Iter = 40  Max. abs. change = 0.00036  Deviance  = 9603.92                                                                                  Iter = 41  Max. abs. change = 0.00034  Deviance  = 9603.92                                                                                  Iter = 42  Max. abs. change = 0.00032  Deviance  = 9603.92                                                                                  Iter = 43  Max. abs. change = 0.00031  Deviance  = 9603.92                                                                                  Iter = 44  Max. abs. change = 0.00029  Deviance  = 9603.92                                                                                  Iter = 45  Max. abs. change = 0.00028  Deviance  = 9603.92                                                                                  Iter = 46  Max. abs. change = 0.00026  Deviance  = 9603.92                                                                                  Iter = 47  Max. abs. change = 0.00025  Deviance  = 9603.92                                                                                  Iter = 48  Max. abs. change = 0.00024  Deviance  = 9603.92                                                                                  Iter = 49  Max. abs. change = 0.00022  Deviance  = 9603.92                                                                                  Iter = 50  Max. abs. change = 0.00021  Deviance  = 9603.92                                                                                  Iter = 51  Max. abs. change = 0.00020  Deviance  = 9603.92                                                                                  Iter = 52  Max. abs. change = 0.00019  Deviance  = 9603.92                                                                                  Iter = 53  Max. abs. change = 0.00018  Deviance  = 9603.92                                                                                  Iter = 54  Max. abs. change = 0.00017  Deviance  = 9603.92                                                                                  Iter = 55  Max. abs. change = 0.00017  Deviance  = 9603.92                                                                                  Iter = 56  Max. abs. change = 0.00016  Deviance  = 9603.92                                                                                  Iter = 57  Max. abs. change = 0.00015  Deviance  = 9603.92                                                                                  Iter = 58  Max. abs. change = 0.00014  Deviance  = 9603.92                                                                                  Iter = 59  Max. abs. change = 0.00014  Deviance  = 9603.92                                                                                  Iter = 60  Max. abs. change = 0.00013  Deviance  = 9603.92                                                                                  Iter = 61  Max. abs. change = 0.00012  Deviance  = 9603.92                                                                                  Iter = 62  Max. abs. change = 0.00012  Deviance  = 9603.92                                                                                  Iter = 63  Max. abs. change = 0.00011  Deviance  = 9603.92                                                                                  Iter = 64  Max. abs. change = 0.00011  Deviance  = 9603.92                                                                                  Iter = 65  Max. abs. change = 0.00010  Deviance  = 9603.92                                                                                  Iter = 66  Max. abs. change = 0.00010  Deviance  = 9603.92                                                                                  </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## using the fitted CDM.obj to avoid extra parameter estimation.</span></span></span>
<span class="r-in"><span><span class="va">Q.MLR.obj</span> <span class="op">&lt;-</span> <span class="fu">validation</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span>, <span class="va">CDM.obj</span>, method <span class="op">=</span> <span class="st">"MLR-B"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> MLR-B  method with  PAA  in  test  level iteration ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =   1/   1, 6 items have changed, ΔAIC=170.79174 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## check QRR</span></span></span>
<span class="r-in"><span><span class="fu"><a href="print.html">print</a></span><span class="op">(</span><span class="fu"><a href="zQRR.html">zQRR</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">Q.MLR.obj</span><span class="op">$</span><span class="va">Q.sug</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] 0.975</span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## also can validate the Q-matrix directly</span></span></span>
<span class="r-in"><span><span class="va">Q.MLR.obj</span> <span class="op">&lt;-</span> <span class="fu">validation</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">dat</span>, <span class="va">MQ</span>, method  <span class="op">=</span> <span class="st">"MLR-B"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> MLR-B  method with  PAA  in  test  level iteration ...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Iter  =   1/   1, 6 items have changed, ΔAIC=170.79174 </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## check QRR</span></span></span>
<span class="r-in"><span><span class="fu"><a href="print.html">print</a></span><span class="op">(</span><span class="fu"><a href="zQRR.html">zQRR</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">Q.MLR.obj</span><span class="op">$</span><span class="va">Q.sug</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] 0.975</span>
<span class="r-in"><span><span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
</code></pre></div>
    </div>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top"><h2 data-toc-skip>Contents</h2>
    </nav></div>
</div>


      <footer><div class="copyright">
  <p></p><p>Developed by Haijiang Qin, Lei Guo.</p>
</div>

<div class="pkgdown">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

      </footer></div>






  </body></html>

